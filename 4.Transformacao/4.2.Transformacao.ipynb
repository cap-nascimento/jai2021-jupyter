{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando os pacotes necess√°rios\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comandos auxiliares\n",
    "from IPython.core.display import display, HTML\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "def display_side_by_side(dfs: list, captions: list):\n",
    "    \"\"\"Display tables side by side to save vertical space\n",
    "    Input:\n",
    "        dfs: list of pandas.DataFrame\n",
    "        captions: list of table captions\n",
    "    \"\"\"\n",
    "    output = \"\"\n",
    "    combined = dict(zip(captions, dfs))\n",
    "    for caption, df in combined.items():\n",
    "        output += df.style.set_table_attributes(\n",
    "            \"style='display:inline'\").set_caption(caption)._repr_html_()\n",
    "        output += \"\\xa0\\xa0\\xa0\"\n",
    "    display(HTML(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforma√ß√£o de Dados\n",
    "---\n",
    "\n",
    "Em geral, ap√≥s um processo de <ins>**Integra√ß√£o de dados**</ins>, colunas com diferentes tipos de dados podem ser geradas. \n",
    "\n",
    "Na <ins>**Transforma√ß√£o de dados**</ins>, o principal objetivo √© exatamente **transformar** esses dados de diferentes formatos em um formato suportado pelo processo de pesquisa, e.g., modelos e algoritmos. \n",
    "\n",
    "A <ins>**Transforma√ß√£o de dados**</ins> possui algumas etapas em comum com a <ins>**Limpeza de dados**</ins>, incluindo t√©cnicas de remo√ß√£o de ru√≠do e discretiza√ß√£o de dados. \n",
    "\n",
    "Portanto, nesta etapa, n√≥s vamos focar nas t√©cnicas que ainda n√£o foram abordadas at√© aqui."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados n√£o padronizados\n",
    "\n",
    "Dados coletados de diferentes fontes podem reunir dados **heterog√™neos**, **n√£o padronizados** e em **diferentes escalas** que muitas vezes necessitam um pr√©-processamento, i.e., uma etapa de escalonamento. \n",
    "\n",
    "O escalonamento de tais dados n√£o √© uma etapa obrigat√≥ria, mas uma boa pr√°tica! \n",
    "\n",
    "> **Mas... Por que precisamos dimensionar as vari√°veis em nosso conjunto de dados?** ü§î\n",
    "\n",
    "> *Muitos algoritmos de Aprendizado de M√°quina apresentam um melhor desempenho quando as vari√°veis num√©ricas de entrada s√£o padronizadas.*\n",
    "\n",
    "Isso inclui algoritmos que usam uma soma ponderada da entrada como, por exemplo, a regress√£o linhear e algoritmos que usam medidas de dist√¢ncia, como os algoritmos K-means e K-NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem algumas t√©cnicas de escalonamento que **transformam** caracter√≠sticas para uma escala, magnitude ou intervalo. \n",
    "\n",
    "Para descrever e exemplicar cada uma delas, utilizaremos o conjunto de atributos num√©ricos que descrevem as m√∫sicas do Spotify. Ou seja, vamos utilizar a tabela `Tracks` do nosso conjunto de dados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo os dados\n",
    "data = pd.read_table('../dataset/spotify_hits_dataset_complete.tsv', encoding='utf-8')\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionando apenas as vari√°veis num√©ricas\n",
    "df = data.select_dtypes(include=['number'])\n",
    "# print(df.columns.values) # imprime as colunas restantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois de selecionar as vari√°veis num√©ricas da nossa tabela, ficamos com as seguintes informa√ß√µes:\n",
    "\n",
    "* `popularity`: *score* de popularidade da track (o valor varia entre 0 e 100, sendo 100 o mais popular)\n",
    "* `track_number`: n√∫mero da track no um √°lbum que ela pertence\n",
    "* `num_artists`: n√∫mero total de int√©rpretes da track\n",
    "* `num_available_markets`: n√∫mero total de pa√≠ses nos quais a track pode ser reproduzida\n",
    "* `duration_ms`: dura√ß√£o da track em milissegundos\n",
    "* [`key` - `tempo`]: caracter√≠sticas ac√∫sticas da track\n",
    "\n",
    "Utilizando esses atributos, n√≥s iremos explorar dois tipos de escalonamento: *Normaliza√ß√£o* e *Padroniza√ß√£o*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NORMALIZA√á√ÉO\n",
    "\n",
    "A *Normaliza√ß√£o* √© uma t√©cnica de escalonamento em que os valores s√£o **deslocados** e **redimensionados** para que fiquem entre 0 e 1. Essa t√©cnica tamb√©m √© conhecida como nomarliza√ß√£o *Min-Max* e √© usada para **transformar** dados em uma <ins>escala semelhante</ins>.\n",
    "\n",
    "A f√≥rmula da *Normaliza√ß√£o* √© dada por:\n",
    "\n",
    "$$X' = \\frac{X - X_{min}}{X_{max} - X_{min}},$$\n",
    "\n",
    "onde $X_{max}$ e $X_{min}$ s√£o os valores m√°ximo e m√≠nimo do atributo, respectivamente.\n",
    "\n",
    "**RESUMINDO**\n",
    "* O valor m√≠nimo da coluna ser√° transformado em 0\n",
    "* O valor m√°ximo da coluna ser√° transformado em 1\n",
    "* Os valores que estiverem entre o valor m√≠nimo e m√°ximo, estar√£o entre o intervalo de 0 a 1\n",
    "\n",
    "#### EXEMPLO\n",
    "No exemplo a seguir, n√≥s iremos normalizar todas as colunas num√©ricas da tabela `Tracks`.\n",
    "\n",
    "Isto √©, para cada atributo (i.e., coluna), n√≥s aplicaremos a f√≥rmula da *Normaliza√ß√£o Min-Max*. \n",
    "\n",
    "Para isso, n√≥s utilizaremos os m√©todos `min()` e `max()` do *pandas*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "df_norm = df.copy()  # c√≥pia do DataFrame\n",
    "\n",
    "# Para cada coluna de df_norm,\n",
    "for coluna in df_norm.columns:\n",
    "    \n",
    "    # Normaliza√ß√£o Min-Max\n",
    "    X = df_norm[[coluna]]\n",
    "    df_norm[[coluna]] = (X - X.min()) / (X.max() - X.min())\n",
    "\n",
    "display_side_by_side([df.head(3), df_norm.head(3)], ['Original', 'Normalizado']) # imprime as 3 primeiras linhas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚ö†Ô∏è OBSERVA√á√ïES\n",
    "\n",
    "Note que n√≥s utilizamos `df_norm[[coluna]]` para selecionar as entradas de uma determinada coluna do nosso *DataFrame*, ao inv√©s de apenas `df_norm[coluna]`.\n",
    "\n",
    "> **Qual a diferen√ßa?** ü§î\n",
    "\n",
    "Utilizando o m√©todo `type()` n√≥s conseguimos verificar o tipo de classe do argumento passado como par√¢metro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df['popularity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df[['popularity']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Portanto, quando utilizamos `[[]]`, √© retornado um *DataFrame* formado apenas pelas colunas passadas por par√¢metro. J√° quando utilizamos `[]`, uma *Series* √© retornada. Neste √∫ltimo caso, apenas uma coluna pode ser passada como par√¢metro. Caso contr√°rio, o seguinte erro ser√° retornado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['popularity','track_number'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['popularity','track_number']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXEMPLO AVAN√áADO\n",
    "\n",
    "Uma outra forma de aplicar a *Normaliza√ß√£o Min-Max* √© utilizando o m√≥dulo `sklearn.preprocessing` e a classe `MinMaxScaler()` da biblioteca *sklearn*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm_2 = df.copy()  # c√≥pia do DataFrame\n",
    "min_max_scaler = MinMaxScaler() # inicializa o transformador\n",
    "\n",
    "# Para cada coluna de df_norm_2,\n",
    "for coluna in df_norm_2.columns:\n",
    "    \n",
    "    # Normaliza√ß√£o Min-Max\n",
    "    df_norm_2[[coluna]] = min_max_scaler.fit_transform(df_norm_2[[coluna]])\n",
    "\n",
    "display_side_by_side([df_norm.head(3), df_norm_2.head(3)], ['Normaliza√ß√£o com pandas', 'Normaliza√ß√£o com sklearn']) # imprime as 3 primeiras linhas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "### PADRONIZA√á√ÉO\n",
    "\n",
    "A *Padroniza√ß√£o* √© outra t√©cnica de escalonamento em que os valores s√£o **centralizados** em torno da m√©dia. Isso significa que a m√©dia de cada atributo se iguala a 0 e a distribui√ß√£o resultante tem um desvio padr√£o igual a 1.\n",
    "\n",
    "A f√≥rmula da *Padroniza√ß√£o* √© dada por:\n",
    "\n",
    "$$X' = \\frac{X - \\mu}{\\sigma},$$\n",
    "\n",
    "onde $\\mu$ e $\\sigma$ s√£o a m√©dia e o desvio padr√£o dos valores do atributo, respectivamente.\n",
    "\n",
    "#### EXEMPLO\n",
    "No exemplo a seguir, n√≥s iremos padronizar todas as colunas num√©ricas da tabela `Tracks`.\n",
    "\n",
    "Isto √©, para cada coluna, os valores ser√£o subtra√≠dos pela m√©dia e divididos pelo desvio padr√£o, aplicando a f√≥rmula descrita anteriormente. Para isso, n√≥s utilizaremos os m√©todos `mean()` e `std()` do *pandas*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_padron = df.copy()  # c√≥pia do DataFrame\n",
    "\n",
    "# Para cada coluna de df_padron,\n",
    "for coluna in df_padron.columns:\n",
    "    \n",
    "    # Padroniza√ß√£o\n",
    "    X = df_padron[[coluna]]\n",
    "    df_padron[[coluna]] = (X - X.mean()) / (X.std())\n",
    "\n",
    "display_side_by_side([df.head(3), df_padron.head(3)], ['Original', 'Padronizado']) # imprime as 3 primeiras linhas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXEMPLO AVAN√áADO\n",
    "\n",
    "Uma outra forma de aplicar a *Padroniza√ß√£o* √© utilizando o m√≥dulo `sklearn.preprocessing` e a classe `StandardScaler()` da biblioteca *sklearn*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_padron_2 = df.copy()  # c√≥pia do DataFrame\n",
    "standard_scaler = StandardScaler() # inicializa o transformador\n",
    "\n",
    "# Para cada coluna de df_padron_2,\n",
    "for coluna in df_padron_2.columns:\n",
    "    \n",
    "    # Padroniza√ß√£o\n",
    "    df_padron_2[[coluna]] = standard_scaler.fit_transform(df_padron_2[[coluna]])\n",
    "\n",
    "display_side_by_side([df_padron.head(3), df_padron_2.head(3)], ['Padroniza√ß√£o com pandas', 'Padroniza√ß√£o com sklearn']) # imprime as 3 primeiras linhas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Engenharia de caracter√≠sticas (Feature Engineering)\n",
    "\n",
    "Outra forma de **transformar** os dados √© atrav√©s da **Engenharia de caracter√≠sticas**. Nesta t√©cnica, caracter√≠sticas de um conjunto de dados podem ser criadas ou removidas para melhorar o desempenho de modelos de Aprendizado de M√°quina. \n",
    "\n",
    "Muitas t√©cnicas de <ins>**Transforma√ß√£o**</ins> e <ins>**Limpeza de dados**</ins> tamb√©m podem ser consideradas **Engenharia de caracter√≠sticas**, incluindo m√©todos de escalonamento, discretiza√ß√£o e redu√ß√£o de dimensionalidade. \n",
    "\n",
    "Portanto, a seguir, n√≥s vamos discutir algumas t√©cnicas que ainda n√£o foram abordadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXEMPLO\n",
    "\n",
    "Para exemplificar, utilizaremos dessa vez as vari√°veis n√£o-num√©ricas da tabela `Tracks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# Selecionando apenas vari√°veis n√£o-num√©ricas\n",
    "df = data.select_dtypes(exclude=['number'])\n",
    "df = df.drop(columns=['song_id', 'song_name', 'artist_id', 'artist_name']) # removendo colunas desnecess√°rias\n",
    "# print(df.columns.values) # imprime as colunas restantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois de selecionar as vari√°veis n√£o-num√©ricas da nossa tabela e de remover colunas desnecess√°rias (i.e., ids e nomes), ficamos com os seguintes dados:\n",
    "\n",
    "* `explicit`: *flag* indicando se a track cont√©m conte√∫do expl√≠cito\n",
    "* `song_type`: se a track √© do tipo *Solo* ou *Collaboration*\n",
    "* `release_date`: data de lan√ßamento da track\n",
    "\n",
    "Utilizando esses atributos, n√≥s iremos explorar duas formas de **Engenharia de caracter√≠sticas**: \n",
    "\n",
    "1. Converter vari√°veis categ√≥ricas em num√©ricas\n",
    "2. Criar novos atributos a partir de vari√°veis de data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DADOS CATEG√ìRICOS\n",
    "\n",
    "Vari√°veis categ√≥ricas s√£o atributos que s√≥ podem assumir um n√∫mero limitado e, geralmente, fixo de valores poss√≠veis. \n",
    "\n",
    "Por exemplo, as m√∫sicas do Spotify cont√™m dados categ√≥ricos para informar se as m√∫sicas s√£o expl√≠citas ou n√£o (`explicit`). \n",
    "\n",
    "Ou seja, cada unidade de observa√ß√£o (i.e., m√∫sica) √© atribu√≠da a um determinado grupo ou categoria nominal (i.e. [`False` ou `True`]). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['explicit']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muitos algoritmos e modelos de Aprendizado de M√°quina t√™m dificuldades em processar dados categ√≥ricos, necessitando alguma forma de **transformar** tais atributos em vari√°veis num√©ricas. \n",
    "\n",
    "Uma das abordagens mais simples e comuns √© converter vari√°veis categ√≥ricas em *dummies* ou indicadores, utilizando a fun√ß√£o `get_dummies()` do *pandas*, conforme o exemplo a seguir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Selecionando os atributos categ√≥ricos\n",
    "df = data[['explicit', 'song_type']].astype('category')\n",
    "\n",
    "# Convertendo em dummies\n",
    "df_dummies = pd.get_dummies(df)\n",
    "\n",
    "display_side_by_side([df.head(3), df_dummies.head(3)], ['Original', 'Transformado']) # imprime as 3 primeiras linhas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O m√©todo `get_dummies()` cria uma nova coluna contendo 0s e 1s para cada categoria poss√≠vel do atributo, onde:\n",
    "\n",
    "* `0`: indica que o dado n√£o √© daquela categoria\n",
    "* `1`: indica que o dado √© daquela categoria \n",
    "\n",
    "Como os dois atributos t√™m apenas duas categorias poss√≠veis, o m√©todo criou duas colunas para cada atributo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VARI√ÅVEIS DE DATA\n",
    "\n",
    "Datas e hor√°rios s√£o fontes valiosas de informa√ß√µes que podem ser usadas em projetos de Ci√™ncia de Dados.\n",
    "\n",
    "Por exemplo, em um modelo de predi√ß√£o de sucesso musical, pode ser √∫til considerar a data de lan√ßamento das m√∫sicas. De fato, existem casos onde o per√≠odo de lan√ßamento influencia diretamente no sucesso musical.\n",
    "**Exemplos:** m√∫sicas de Carnaval, m√∫sicas natalinas, m√∫sicas da Copa, etc.\n",
    " \n",
    "No entanto, tecnicamente, as vari√°veis de data requerem alguma **Engenharia de caracter√≠sticas** para **transform√°-las** em dados num√©ricos antes de serem usadas pelos algoritmos de Aprendizado de M√°quina. \n",
    "\n",
    "#### EXEMPLO\n",
    "\n",
    "Para exemplificar, utilizaremos a data de lan√ßamento das m√∫sicas do Spotify (`release_date`) convertendo em formatos mais amig√°veis, extraindo recursos e criando novas vari√°veis que podem ser usadas na an√°lise de algum modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# Selecionando o id, nome e a data de lan√ßamento das tracks\n",
    "df = data[['song_id', 'song_name', 'release_date']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O primeiro passo do processamento de datas √© verificar se as vari√°veis est√£o em formato `datetime`.\n",
    "Para isso, podemos utilizar o `dtypes` para verificar o tipo de cada coluna do nosso *DataFrame*: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes # verificando o tipo das colunas de df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos notar, o tipo da vari√°vel `release_date` √© `object` e n√£o `datetime`. Portanto, precisamos convert√™-la antes de passar para a **Engenharia de caracter√≠sticas**. Para isso, usaremos o m√©todo `to_datetime()` do *pandas*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['release_date'] = pd.to_datetime(df['release_date']) # convertendo a data de lan√ßamento em 'datetime'\n",
    "df.dtypes # verificando o tipo das colunas de df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A biblioteca *pandas* disponibiliza diversas propriedades de vari√°veis `datetime`, incluindo:\n",
    "\n",
    "* `year`: o ano da `datetime`\n",
    "* `month`: o m√™s da `datetime`, onde `January=1` e `December=12`\n",
    "* `day`: o dia da `datetime`\n",
    "\n",
    "Ent√£o, depois de converter nossa vari√°vel para o tipo `datetime`, utilizando essas propriedades n√≥s podemos quebrar a data de lan√ßamento das m√∫sicas em dia, m√™s e ano: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day'] = df['release_date'].dt.day  # dia\n",
    "df['month'] = df['release_date'].dt.month  # m√™s\n",
    "df['year'] = df['release_date'].dt.year  # ano\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al√©m dessas propriedades b√°sicas, existem outras informa√ß√µes que podemos extrair da nossa vari√°vel `datetime`.\n",
    "\n",
    "Por exemplo, a semana do ano, o dia da semana e o trimestre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['week'] = df['release_date'].dt.week  # semana do ano\n",
    "df['dayofweek_index'] = df['release_date'].dt.dayofweek  # dia da semana (√≠ndice)\n",
    "df['dayofweek_name'] = df['release_date'].dt.day_name() # dia da semana (nome)\n",
    "df['quarter'] = df['release_date'].dt.quarter # trimestre\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem ainda diversas outras propriedades poss√≠veis de se obter que voc√™ pode encontrar atrav√©s da documenta√ß√£o do *pandas*: [DatetimeIndex](https://pandas.pydata.org/docs/reference/api/pandas.DatetimeIndex.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Dados desbalanceados\n",
    "\n",
    "A √∫ltima t√©cnica de <ins>**Transforma√ß√£o de dados**</ins> que vamos analisar √© a o **Balanceamento de dados**.  \n",
    "\n",
    "Especificamente, n√≥s podemos dizer que os dados est√£o desbalanceados quando existe uma **despropor√ß√£o** de observa√ß√µes de cada classe do nosso modelo. Os impactos do desbalanceamento s√£o impl√≠citos, ou seja, n√£o geram um erro imediato ao construir e executar um modelo. \n",
    "\n",
    "‚ö†Ô∏è Por√©m, os resultados podem ser **ilus√≥rios**. \n",
    "\n",
    "Se o sinal da classe *majorit√°ria* for **bem maior** do que o sinal da classe *minorit√°ria*, o classificador pode gerar uma alta precis√£o geral, uma vez que o modelo provavelmente ir√° prever a maioria das amostras pertencentes √† classe majorit√°ria. O que n√£o quer dizer que o modelo teve um bom desempenho no geral. \n",
    "\n",
    "Por exemplo, em um problema de duas classes com uma distribui√ß√£o de classe de `90:10`, o desempenho ao classificar as classes *majorit√°rias* ser√° **nove** vezes maior do que o desempenho em exemplos de classes *minorit√°rias*.\n",
    "\n",
    "> **Achou confuso?** Vamos para um exemplo ent√£o! üòâ\n",
    "\n",
    "### EXEMPLO\n",
    "\n",
    "Para exemplificar, suponha que voc√™ tenha um modelo de classifica√ß√£o onde voc√™ precisa prever se uma m√∫sica ser√° uma colabora√ß√£o ou n√£o, com base em suas caracter√≠sticas ac√∫sticas. \n",
    "\n",
    "Resumindo, voc√™ teria um modelo de classifica√ß√£o bin√°ria com duas classes: \n",
    "\n",
    "* `Collaboration`\n",
    "* `Solo`\n",
    "\n",
    "Para come√ßar, ent√£o, vamos selecionar apenas as colunas referentes aos nossos preditores (i.e., as caracter√≠sticas ac√∫sticas das m√∫sicas) e a nossa vari√°vel *target*, i.e., `song_type`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    'duration_ms', 'key', 'mode', 'time_signature', 'acousticness',\n",
    "    'danceability', 'energy', 'instrumentalness', 'liveness', 'loudness',\n",
    "    'speechiness', 'valence', 'tempo', 'song_type'\n",
    "]\n",
    "\n",
    "# Selecionando algumas colunas da tabela Tracks\n",
    "data = pd.read_table('../dataset/spotify_hits_dataset_complete.tsv', encoding='utf-8', usecols=cols)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Note que a vari√°vel *target* `song_type` est√° em formato n√£o-num√©rico. Como vimos anteriormente, n√≥s **precisar√≠amos** transform√°-la em uma vari√°vel num√©rica caso esta vari√°vel fosse usada em algum algoritmo de Aprendizado de M√°quina. \n",
    "\n",
    "No entanto, neste caso, tal vari√°vel corresponde ao que queremos prever (i.e., vari√°vel *target*). Portanto, n√£o precisamos aplicar nenhum tratamento aqui."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos, ent√£o, seguir para a verifica√ß√£o do balanceamento das classes (`Solo` e `Collaboration`). \n",
    "Para isso, usamos a fun√ß√£o `value_counts()` que computa os valores √∫nicos de cada categoria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = data['song_type'].value_counts() # quantidade de cada classe\n",
    "p = data['song_type'].value_counts(normalize=True) # porcentagem de cada classe\n",
    "pd.concat([c,round(p*100)], axis=1, keys=['counts', '%'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que cerca de 41% das observa√ß√µes (i.e., 532 m√∫sicas) s√£o colabora√ß√µes. \n",
    "\n",
    "Portanto, se tiv√©ssemos que prever sempre m√∫sicas `Solo` obter√≠amos uma precis√£o de 59%.\n",
    "\n",
    "A seguir, iremos testar esse cen√°rio criando um modelo linear de regress√£o log√≠stica, usando o m√≥dulo `sklearn.linear_model` e a classe `LogisticRegression`. Para isso, n√≥s seguimos os seguintes passos:\n",
    "\n",
    "1. Separamos o conjunto de predidores (X) da nossa vari√°vel *target* (y);\n",
    "2. Treinamos o modelo de regress√£o log√≠stica com os dois conjuntos X e y;\n",
    "3. Realizamos a predi√ß√£o utilizando o mesmo conjunto de preditores X; e, por fim, \n",
    "4. Imprimimos os resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primeiro passo\n",
    "y = data.song_type # vari√°vel target\n",
    "X = data.drop('song_type', axis=1) # conjunto de preditores\n",
    "\n",
    "# segundo passo\n",
    "logr = LogisticRegression().fit(X, y) # treinando o modelo de regress√£o log√≠stica\n",
    "\n",
    "# terceiro passo\n",
    "pred_y_0 = logr.predict(X)\n",
    "accuracy = round(accuracy_score(pred_y_0, y)*100)\n",
    "\n",
    "# imprimindo os resultados\n",
    "print(f'{accuracy}%') # acur√°cia do modelo\n",
    "print(np.unique(pred_y_0)) # classes previstas pelo modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver pelo resultado, nosso modelo apresentou uma precis√£o geral de 59%; ou seja, como consequ√™ncia dos dados desbalanceados, o modelo prev√™ apenas uma classe. \n",
    "\n",
    "Em outras palavras, ele **ignora** a classe minorit√°ria (i.e., `Collaborations`) em favor da classe majorit√°ria (i.e., `Solo`). \n",
    "\n",
    "> **Ent√£o, como solucionar este problema?** ü§î\n",
    "\n",
    "Uma t√©cnica amplamente adotada para lidar com conjuntos de dados desbalanceados √© a **Reamostragem**. \n",
    "\n",
    "#### REAMOSTRAGEM DE CLASSES\n",
    "\n",
    "Esta t√©cnica pode ser dividida em duas abordagens diferentes: \n",
    "\n",
    "* **Downsampling:** consiste em **remover** amostras da classe *majorit√°ria* \n",
    "* **Oversampling:** consiste em **adicionar** mais exemplos da classe *minorit√°ria*  \n",
    "\n",
    "<img src=\"figure6.png\" alt=\"schema\" style=\"width: 700px;\" class=\"center\"/>\n",
    "\n",
    "A seguir, vamos aplicar as duas t√©cnicas no nosso exemplo de predi√ß√£o de colabora√ß√µes.\n",
    "\n",
    "---\n",
    "\n",
    "**1. Downsampling** \n",
    "\n",
    "Come√ßamos aplicando a t√©cnica de *Downsampling*, que remove aleatoriamente observa√ß√µes da classe majorit√°ria para evitar que seu sinal domine o algoritmo de aprendizagem. Para isso, seguimos alguns passos:\n",
    "\n",
    "1. Separamos as observa√ß√µes de cada classe em diferentes *DataFrames*;\n",
    "2. Realizamos uma amostragem da classe majorit√°ria sem substitui√ß√£o para corresponder ao n√∫mero total da classe minorit√°ria; e, por fim,  \n",
    "3. Concatenamos o *DataFrame* da classe minorit√°ria com o *DataFrame* resultante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primeiro passo\n",
    "df_majority = data[data.song_type == 'Solo'] # classe majorit√°ria\n",
    "df_minority = data[data.song_type == 'Collaboration'] # classe minorit√°ria\n",
    "\n",
    "# segundo passo\n",
    "df_majority_downsampled = resample(\n",
    "    df_majority, replace=False,  # amostra sem substitui√ß√£o\n",
    "    n_samples=len(df_minority),  # para corresponder √† classe minorit√°ria\n",
    "    random_state=123)  # garantindo reprodutibilidade\n",
    "\n",
    "# terceiro passo\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "df_downsampled.song_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_downsampled.shape[0], data.shape[0]) # comparando o tamanho dos DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Note que, apesar do novo *DataFrame* ter menos observa√ß√µes do que o original, a propor√ß√£o das duas classes est√° **balanceada**. \n",
    "\n",
    "Agora, podemos treinar novamente o modelo de regress√£o log√≠stica! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primeiro passo\n",
    "y = df_downsampled.song_type # vari√°vel target\n",
    "X = df_downsampled.drop('song_type', axis=1) # conjunto de preditores\n",
    "\n",
    "# segundo passo\n",
    "logr = LogisticRegression().fit(X, y) # treinando o modelo de regress√£o log√≠stica\n",
    "\n",
    "# terceiro passo\n",
    "pred_y_1 = logr.predict(X)\n",
    "accuracy = round(accuracy_score(pred_y_1, y)*100)\n",
    "\n",
    "# imprimindo os resultados\n",
    "print(f'{accuracy}%') # acur√°cia do modelo\n",
    "print(np.unique(pred_y_1)) # classes previstas pelo modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obeserve que, ap√≥s o balanceamento, o modelo consegue prever as duas classes e, apesar de apresentar uma menor precis√£o (49%), o modelo apresenta uma m√©trica de avalia√ß√£o mais significativa. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Oversampling** \n",
    "\n",
    "Vamos, agora, aplicar a t√©cnica de *Oversampling*, que replica aleatoriamente observa√ß√µes da classe minorit√°ria. Para isso, seguimos alguns passos:\n",
    "\n",
    "1. Separamos as observa√ß√µes de cada classe em diferentes *DataFrames*;\n",
    "2. Realizamos a replica√ß√£o da classe minorit√°ria para corresponder ao n√∫mero total da classe majorit√°ria; e, por fim,  \n",
    "3. Concatenamos o *DataFrame* da classe majorit√°ria com o *DataFrame* resultante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primeiro passo\n",
    "df_majority = data[data.song_type == 'Solo'] # classe majorit√°ria\n",
    "df_minority = data[data.song_type == 'Collaboration'] # classe minorit√°ria\n",
    "\n",
    "# segundo passo\n",
    "df_minority_oversampled = resample(\n",
    "    df_minority, # replica√ß√£o aleat√≥ria\n",
    "    n_samples=len(df_majority),  # para corresponder √† classe majorit√°ria\n",
    "    random_state=123)  # garantindo reprodutibilidade\n",
    "\n",
    "# terceiro passo\n",
    "df_oversampled = pd.concat([df_minority_oversampled, df_majority])\n",
    "df_oversampled.song_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_oversampled.shape[0], data.shape[0]) # comparando o tamanho dos DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que, desta vez, o novo *DataFrame* possui menos observa√ß√µes do que o original ap√≥s a replica√ß√£o das classes minorit√°rias.\n",
    "\n",
    "Agora, podemos treinar novamente o modelo de regress√£o log√≠stica!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primeiro passo\n",
    "y = df_oversampled.song_type # vari√°vel target\n",
    "X = df_oversampled.drop('song_type', axis=1) # conjunto de preditores\n",
    "\n",
    "# segundo passo\n",
    "logr = LogisticRegression().fit(X, y) # treinando o modelo de regress√£o log√≠stica\n",
    "\n",
    "# terceiro passo\n",
    "pred_y_1 = logr.predict(X)\n",
    "accuracy = round(accuracy_score(pred_y_1, y)*100)\n",
    "\n",
    "# imprimindo os resultados\n",
    "print(f'{accuracy}%') # acur√°cia do modelo\n",
    "print(np.unique(pred_y_1)) # classes previstas pelo modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais uma vez, ap√≥s o balanceamento, o modelo conseguiu prever as duas classes apresentando uma m√©trica de precis√£o mais significativa. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclus√£o\n",
    "\n",
    "Este notebook apresentou como fazer a transforma√ß√£o de dados de diferentes formatos a partir de diversas abordagens.\n",
    "\n",
    "üîé **Se interessou?** D√™ uma olhada na documenta√ß√£o da biblioteca *sklearn* para informa√ß√µes extras sobre transforma√ß√£o de dados:\n",
    "[Dataset transformations](https://scikit-learn.org/stable/data_transforms.html)\n",
    "\n",
    "---\n",
    "\n",
    "O pr√≥ximo notebook ([4.3.Reducao.ipynb](4.3.Reducao.ipynb)) apresenta como aplicar t√©cnicas de redu√ß√£o de dados para auxiliar a an√°lise de dados com alta dimensionalidade."
   ]
  }
 ],
 "metadata": {
  "julynter-results": {
   "filteredId": [],
   "filteredIndividual": [],
   "filteredRestart": [],
   "filteredType": [],
   "hash": "5262264cd9b5bca8895a3c3c49a809815b06710a",
   "visible": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
