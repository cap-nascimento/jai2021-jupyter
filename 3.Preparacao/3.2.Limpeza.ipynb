{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T20:07:24.597099Z",
     "start_time": "2021-04-20T20:07:24.220697Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# Importar os pacotes necessários\n",
    "import pandas as pd\n",
    "#from fuzzywuzzy import fuzz\n",
    "#from itertools import combinations\n",
    "\n",
    "pd.set_option('display.max_columns', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparação de Dados para Ciência\n",
    "\n",
    "Em Ciência de Dados, dados de qualidade são pré-requisito para pesquisas válidas, descobertas significativas, modelos de Aprendizado de Máquina, entre outros. Porém, no mundo real, dados brutos costumam ser incompletos, ruidosos, inconsistentes e, às vezes, estão em formato inutilizável. Portanto, antes de alimentá-los a modelos (e outras etapas de pesquisa), é fundamental averiguar a integridade de dados e identificar possíveis problemas. Este processo é denominado pré-processamento de dados.\n",
    "\n",
    "Essencialmente, preparar dados significa adequá-los para servirem de entrada nos processos da pesquisa. Existem muitas técnicas de pré-processamento que, geralmente, acontecem em etapas organizadas nas seguintes categorias: Limpeza de Dados, Integração de Dados, Transformação de Dados, e Redução de Dados. As etapas do préprocessamento não são mutuamente exclusivas e são altamente dependentes do conjunto de dados; ou seja, podem trabalhar em conjunto, mas não são obrigatórias.\n",
    "\n",
    "Em particular, a Limpeza de dados pode remover ruído e corrigir inconsistências nos dados. A Integração de dados mescla dados de várias fontes em um armazenamento de dados coerente, como um armazém de dados. Transformações de dados, como normalização, podem melhorar a precisão e eficiência de algoritmos que envolvem medições de distância. Então, a Redução de dados pode diminuir o tamanho dos dados agregando ou eliminando recursos redundantes. Através de exemplos com dados reais, esta seção define e descreve cada uma dessas etapas técnicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpeza de Dados\n",
    "\n",
    "A limpeza de dados é o processo de **detecção e correção de registros incorretos ou corrompidos** em um conjunto de dados. \n",
    "\n",
    "Após a identificação de registros incorretos ou corrompidos, podemos:\n",
    "\n",
    "- substituir\n",
    "- modificar \n",
    "- excluir partes incompletas, imprecisas ou irrelevantes. \n",
    "\n",
    "> 💡 Em geral, a limpeza de dados leva a uma **sequência de tarefas** que visam melhorar a qualidade dos dados. Algumas dessas tarefas incluem não só lidar com dados ausentes e duplicados, mas também remover dados ruidosos, inconsistentes e outliers. \n",
    "\n",
    "Para começar o processo de limpeza, primeiro vamos realizar a importação dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T20:07:25.366638Z",
     "start_time": "2021-04-20T20:07:25.331640Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ler a tabela modificada\n",
    "df = pd.read_csv('../dataset/spotify_artists_info_edited.csv', sep='\\t', encoding='utf-8')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados ausentes \n",
    "\n",
    "Representam um obstáculo para a criação da maioria dos modelos de Aprendizado de Máquina e outras análises. Portanto, é necessário identificar **campos para os quais não há dados** e, em seguida, compensá-los adequadamente. \n",
    "\n",
    "Dados ausentes podem ocorrer quando nenhuma informação é fornecida para um ou mais registros (ou atributos inteiros) da base de dados. Em um _**Pandas DataFrame**_, os dados ausentes são representados como **`None`** ou **`NaN`** (_Not a Number_).\n",
    "\n",
    "> ⚠️ **`NaN`** é o marcador de valor ausente **padrão** por razões de velocidade e conveniência computacional. \n",
    "\n",
    "Após importar pacotes necessários e carregar o conjunto de dados, inicia-se o processo de limpeza de dados. Para facilitar a detecção, o __Pandas__ fornece a função **`isna()`** para identificar valores ausentes em um _DataFrame_.\n",
    "\n",
    "A função retorna uma **matriz booleana** indicando se cada elemento correspondente está faltando **(`True`) ou não (`False`)**. O exemplo a seguir apresenta o uso desta função no *DataFrame* `df` e exibe seu resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta célula identifica valores ausentes no Dataframe\n",
    "df.isna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De acordo com o resultado da célula anterior, o conjunto de dados está **aparentemente** completo. \n",
    "\n",
    "> ⚠️ Porém, isso não é suficiente para descartar a hipótese de que **existem dados ausentes.**\n",
    "\n",
    "Para uma melhor averiguação, pode-se resumir cada coluna no _DataFrame booleano_ somando os valores **False = 0**  e **True = 1** . Tal processo retorna o número de valores ausentes no _DataFrame_. \n",
    "\n",
    "Também pode-se dividir cada valor pelo número total de linhas, resultando na **porcentagem de tais ausências**, conforme o exemplo a seguir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T20:09:52.004424Z",
     "start_time": "2021-04-20T20:09:51.979435Z"
    },
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# Calcular o total e a porcentagem de valores ausentes\n",
    "\n",
    "## retorna o total de valores ausentes de cada coluna\n",
    "num_ausentes = df.isna().sum() \n",
    "\n",
    "## retorna a porcentagem de valores ausentes de cada coluna\n",
    "porc_ausentes = df.isna().sum() * 100 / len(df)\n",
    "\n",
    "# Cria um DataFrame com as informações computadas acima\n",
    "df_ausentes = pd.DataFrame({\n",
    "    'Coluna': df.columns,\n",
    "    'Dados ausentes': num_ausentes,\n",
    "    'Porcentagem': porc_ausentes\n",
    "})\n",
    "df_ausentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O _DataFrame_ resultante contém os seguintes dados **ausentes**:\n",
    "- **62** popularidades\n",
    "- **40** listas de gêneros e \n",
    "- **10** url de imagens\n",
    "\n",
    "O que resulta em 9,9%, 6,4% e 1,6% as porcentagens de registros ausentes de cada coluna, respectivamente. \n",
    "\n",
    "Após essa identificação, é necessário **tratar esses dados**. \n",
    "\n",
    "> 💡 A abordagem mais simples é **eliminar todos os registros que contenham valores ausentes**. \n",
    "No _Pandas_, o método **dropna()** permite analisar e descartar linhas/colunas com valores nulos. \n",
    "\n",
    "O parâmetro **axis** determina a dimensão em que a função atuará: \n",
    "- **axis = 0** remove todas as _linhas_ que contêm valores nulos\n",
    "- **axis = 1** remove as _colunas_ que contêm valores nulos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminando as linhas onde pelo menos um elemento está faltando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "novo_df = df.dropna(axis=0) # retorna para um novo dataframe, sem as linhas que contém nulos\n",
    "\n",
    "print(f\"\"\"\\\n",
    "Nº de linhas do DF original: {len(df)}\n",
    "Nº de linhas do DF novo: {len(novo_df)}\n",
    "Nº de linhas com pelo menos 1 valor ausente: {\n",
    "(len(df) - len(novo_df))}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminando as colunas onde pelo menos um elemento está faltando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "novo_df = df.dropna(axis=1) # retorna para um novo dataframe, sem as colunas que contém nulos\n",
    "\n",
    "print(f\"\"\"\\\n",
    "Nº de colunas do DF original: {len(df.columns)}\n",
    "Nº de colunas do DF novo: {len(novo_df.columns)}\n",
    "Nº de colunas com pelo menos 1 valor ausente: {\n",
    "(len(df.columns) - len(novo_df.columns))}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⚠️ ATENÇÃO!⚠️\n",
    "\n",
    "O código dos exemplos anteriores **removeram as linhas/colunas onde pelo menos um elemento está faltando**. \n",
    "\n",
    "Ambas abordagens são particularmente vantajosas para amostras de **grande volume de dados**, onde os valores podem ser descartados sem distorcer significativamente a interpretação. Em geral, a estratégia de exclusão é utilizada quando o problema de falta de dados ocorre na **maioria** das linhas ou colunas do conjunto de dados. Por exemplo, se mais de 75% das linhas correspondentes a um atributo (coluna) são ausentes, é melhor remover tal atributo.\n",
    "\n",
    "> 💡  Vale lembrar que esse valor de 75% de dados ausentes **não é uma regra**, e não há uma receita de bolo: tudo vai depender dos seus dados e dos seus objetivos.\n",
    "\n",
    "> ⚠️ No entanto, apesar de ser uma solução simples, ela **apresenta o risco de perder dados potencialmente úteis**.\n",
    "\n",
    "### Solução Alternativa - Imputar Dados 🎲🎲\n",
    "\n",
    "Uma alternativa mais confiável para lidar com dados ausentes é a **imputação**. Em vez de descartar tais dados, a imputação procura **substituir seus valores por outros**. Nessa abordagem, os valores ausentes são inferidos a partir dos dados existentes. \n",
    "\n",
    "> 💡 Existem várias maneiras de imputar os dados, sendo a imputação por valor constante ou por estatísticas básicas **(média, mediana ou moda)** as mais simples.\n",
    "\n",
    "No exemplos a seguir, os valores de colunas ausentes serão substituídos utilizando a função `fillna()`. Mas antes, vamos criar uma cópia do _DataFrame_ original para trabalharmos com ele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma cópia do DataFrame original\n",
    "copia_df = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputando dados na coluna _`image_url`_\n",
    "\n",
    "Substituindo todos os dados ausentes da coluna **'image_url'** por um valor estático \n",
    "\n",
    "Por exemplo, podemos inserir a url de imagem _default_ em todas as linhas em que este campo encontra-se nulo.\n",
    "\n",
    "![Imagem](./img/default-user-icon-1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copia_df[\"image_url\"].fillna('https://icon-library.com/icon/default-user-icon-1.html', inplace=True)\n",
    "copia_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputando dados na coluna _`popularity`_\n",
    "\n",
    "Neste caso, como esta é uma coluna numérica, podemos substituir todos os dados ausentes da coluna _popularity_ pela **média dos valores presentes na coluna**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substitui NaNs pela média de valores presentes\n",
    "copia_df['popularity'].fillna(copia_df['popularity'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 💡 OBS: Quando **inplace='True'** é passado, os dados são alterados no próprio dataframe (não retorna nada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "# Coluna Popularity do Dataframe Original\n",
    "df['popularity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "# Coluna Popularity do Dataframe após a Imputação da média da popularidade em campos NaN\n",
    "copia_df['popularity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputando dados na coluna _`genres`_\n",
    "\n",
    "Neste caso, como não seria viável analisar cada artista separadamente para tentar inferir seu gênero musical, uma opção é substituir os valores ausentes por _**unknown**_.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substituir NaNs por 'unknown'\n",
    "copia_df['genres'].fillna('unknown', inplace=True)\n",
    "\n",
    "# Coluna Genres após imputação\n",
    "copia_df['genres'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 💡 Para saber quais os índices que contém o valor 'unknown':\n",
    "<br>\n",
    "``\n",
    "np.where(copia_df['genres']=='unknown')\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificando se todos os valores 'NaN' foram devidamente preenchidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula o total e a % de valores ausentes\n",
    "num_ausentes = copia_df.isna().sum() \n",
    "porc_ausentes = copia_df.isna().sum() * 100 / len(copia_df)\n",
    "\n",
    "# DataFrame com as informações computadas acima\n",
    "df_ausentes = pd.DataFrame({\n",
    "    'Coluna': copia_df.columns,\n",
    "    'Dados ausentes': num_ausentes,\n",
    "    'Porcentagem': porc_ausentes\n",
    "})\n",
    "df_ausentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também existem várias técnicas de imputação avançadas cuja escolha depende da utilização dos dados, por exemplo, depende de **um modelo de aprendizado de máquina** para inserir e avaliar com precisão os dados ausentes. A imputação múltipla e modelos preditivos podem ser mais precisos, e assim são mais comuns do que métodos mais simples. \n",
    "\n",
    "> 💡 **não existe uma maneira ideal de compensar os valores ausentes**, pois cada estratégia pode ter um desempenho melhor ou pior dependendo do conjunto de dado e dos tipos de dados ausentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados ruidosos 🎲📢\n",
    "\n",
    "São dados que fornecem informações adicionais porém **sem sentido**, chamadas de ruído. Geralmente são gerados por alguma falha na coleta de dados, erros de entrada de dados, entre outros. Dados com ruído podem prejudicar resultados de análises e de modelos, como os de aprendizado de máquina, por exemplo. \n",
    "\n",
    "> 💡 Algumas soluções para tal problema incluem diferentes abordagens: tais como o método de **Binning**, **Regressão** e e algoritmos de agrupamento de dados (**Clustering**). \n",
    "\n",
    "Aqui, o foco é o método de **Binning**, uma técnica de **suavização de dados** para reduzir os efeitos de pequenos erros de observação. \n",
    "\n",
    "Os dados originais são divididos em segmentos de tamanhos iguais (_**bins**_) e, em seguida, são substituídos por um valor geral calculado para cada intervalo. Cada segmento é tratado separadamente, onde a substituição de valores pode ser realizada através de valores médios ou limites. \n",
    "\n",
    "> ⚠️ No _Pandas_, o método Binning usa as funções `cut()` e `qcut()`, que parecem iguais mas possuem diferenças."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `qcut`\n",
    "\n",
    "De acordo com a documentação, `qcut()` é uma **função de discretização baseada em quantis**: ela procura dividir os dados em _bins_ usando percentis com base na distribuição da amostra. \n",
    "\n",
    "A maneira mais simples de usá-la é **definir o número de quantis** e deixar que o _Pandas_ descubra como dividir os dados. \n",
    "\n",
    "O exemplo a seguir discretiza a variável **_followers_** de duas maneiras diferentes: \n",
    "- criando cinco _bins_ de mesmo tamanho\n",
    "- configurando três quantis rotulados como \"alto\", \"médio\" e \"baixo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretiza a variável 'followers' em 5 intervalos de tamanhos iguais\n",
    "df['qcut_1'] = pd.qcut(df['followers'], q=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretiza a variável 'followers' setamos três quantis rotulados como 'alto', 'médio' e 'baixo'\n",
    "df['qcut_2'] = pd.qcut(df['followers'],  q=[0, .3, .7, 1], labels=[\"baixo\", \"médio\", \"alto\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibir o resultado das divisões\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `cut`\n",
    "\n",
    "Pode-se utilizar a função `cut()` para segmentar e ordenar os dados em _bins_. Enquanto `qcut()` calcula o tamanho de cada _bin_, garantindo que a distribuição dos dados nos compartimentos seja igual, a função `cut()` **define bordas exatas dos compartimentos**. \n",
    "\n",
    "> ⚠️ Neste caso não há garantia sobre a distribuição de itens em cada _bin_. \n",
    "\n",
    "O exemplo a seguir corta os dados da variável _**followers**_ em quatro bins de tamanhos iguais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretiza a variável 'followers' em 4 bins\n",
    "df['cut_1'] = pd.cut(df['followers'], bins=4)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conforme mostrado a seguir, se você deseja uma distribuição igual dos valores em cada compartimento, use `qcut()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "df['qcut_1'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso contrário, se você quiser definir seus próprios intervalos numéricos de categorias, use a função `cut()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "df['cut_1'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "\n",
    "São amostras de dados que são **claramente diferentes da tendência central**. Geralmente são criados por erros de coleta ou entrada de dados, e podem facilmente **produzir valores discrepantes** interferindo na qualidade de análises. \n",
    "\n",
    "> 💡 A maneira mais simples de identificar outliers é **observar os valores máximos e mínimos** em cada variável para ver se eles estão muito fora da curva normal. \n",
    "\n",
    "\n",
    "O exemplo a seguir utiliza a função `describe()` para gerar estatísticas descritivas do conjunto de dados, incluindo os valores máximos e mínimos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 10)\n",
    "\n",
    "# Gera estatísticas descritivas das variáveis numéricas\n",
    "df.describe().round().transpose() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 💡 Para arredondar valores, use a função `.round()`\n",
    "<br>\n",
    "Para exibir a matriz transposta: `.transpose()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para a coluna _followers_, o valor máximo é 77.7 milhões de seguidores, enquanto o quartil de 75% é apenas 48 milhões. Portanto, artistas com mais de 77 milhões de seguidores **podem ser outliers**. \n",
    "\n",
    "Essa verificação geral é melhor realizada através da representação gráfica dos dados numéricos por meio de seus quartis. Para isso, pode-se utilizar **box plots**, onde os valores discrepantes são plotados como pontos individuais. \n",
    "\n",
    "> 💡 Este tipo de visualização será melhor abordado mais adiante!\n",
    "\n",
    "O gráfico do exemplo a seguir mostra a distribuição da variável **_followers_**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# Plota um boxplot da coluna 'followers'\n",
    "df.boxplot(column=['followers'], figsize=(15, 3), vert=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O exemplo ilustra que existem inúmeros pontos individuais (outliers) entre aproximadamente **12** a **78 milhões** (observe que o eixo x está em dezenas de milhões). \n",
    "\n",
    "Embora tenha sido fácil detectar tais valores discrepantes, é preciso determinar as soluções adequadas para tratá-los. Assim como no caso de dados ausentes, o tratamento de outliers **depende muito do conjunto de dados e do objetivo do projeto**. Soluções possíveis incluem:\n",
    "\n",
    "- manter\n",
    "- ajustar \n",
    "- ou apenas remover os dados discrepantes\n",
    "\n",
    "Uma técnica comum para a remoção de outliers é o método de **Z-score**, que considera como outliers e remove valores a uma determinada quantidade de desvios padrões da média. A quantidade desses desvios pode variar conforme o tamanho da amostra.\n",
    "\n",
    "No exemplo a seguir, para identificar e remover os outliers da coluna **_followers_**, usou-se os z-scores de seus registros com a quantidade de desvios configurada para três. Para a obtenção dos z-scores, foi usado o módulo `stats` da biblioteca `SciPy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa os pacotes necessários\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Calcula os z-scores dos valores da coluna 'followers'\n",
    "z_scores = stats.zscore(df['followers'])\n",
    "\n",
    "# Converte cada elemento em z_scores em seu valor absoluto\n",
    "# função `abs` de Numpy\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "\n",
    "# Filtra o DataFrame original com uma quantidade de desvios padrões < 3\n",
    "novo_df = df[abs_z_scores < 3]\n",
    "\n",
    "print(f'{(len(df) - len(novo_df))} foram outliers removidos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados duplicados\n",
    "\n",
    "Aparecem em muitos contextos, especialmente durante a entrada ou coleta de dados. Por exemplo, ao usar um _web scraper_, a mesma página web pode ser coletada mais de uma vez, ou as mesmas informações podem estar em páginas diferentes. \n",
    "\n",
    "> ⚠️ Independente da causa, a duplicação de dados **pode levar a conclusões incorretas**, onde algumas observações podem ser consideradas mais comuns do que realmente são. \n",
    "\n",
    "O exemplo a seguir mostra quantas linhas estão duplicadas em cada coluna do conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# Calcula o total de linhas duplicadas em cada coluna do nosso DataFrame\n",
    "# Para cada coluna,\n",
    "for coluna in df.columns:\n",
    "    # Seleciona linhas duplicadas e as insire em um novo Dataframe\n",
    "    duplicatas_df = df[df.duplicated(coluna)]\n",
    "    \n",
    "    # Imprime o tamanho do novo DataFrame (i.e., o número de linhas duplicadas)\n",
    "    print(f\"Total de linhas duplicadas \"\n",
    "          f\"na {coluna}: {len(duplicatas_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que apenas duas colunas do _DataFrame_ não possuem duplicatas: **artist_id** e **followers**. \n",
    "\n",
    "Além disso, observem que há **duas cópias do nome de um mesmo artista**. Essa duplicidade de dados é a categoria mais simples de duplicatas: são cópias exatamente iguais de um mesmo registro. Para resolver, basta identificar os valores idênticos e removê-los. \n",
    "\n",
    "O _Pandas_ fornece o método **`drop_duplicates()`** que retorna um novo _DataFrame_ com linhas duplicadas removidas, como no exemplo a seguir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retorna um novo DataFrame com linhas duplicadas removidas\n",
    "novo_df = df.drop_duplicates()\n",
    "\n",
    "# Calcula o total de linhas duplicadas do novo DataFrame\n",
    "duplicatas_df = novo_df[novo_df.duplicated()]\n",
    "print(f\"Total de linhas duplicadas: {len(duplicatas_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com apenas uma lista de registros com duplicatas, a melhor e mais simples solução é geralmente a remoção. Porém, com dados tabulares, a melhor solução é remover os dados duplicados com base em um **conjunto de identificadores exclusivos**. \n",
    "\n",
    "Por exemplo, existe a coluna de identificadores únicos dos artistas (**_`artist_id`_**), que facilita analisar se o nome duplicado identificado pode ser descartado, conforme o seguinte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 5)\n",
    "# Extrai o nome duplicado\n",
    "nome_duplicado = df[df.duplicated(['name'])].name\n",
    "\n",
    "# Localiza as linhas onde 'name' é igual ao nome duplicado\n",
    "df.loc[df['name'].isin(nome_duplicado)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os resultados mostram dois artistas com o nome **Niack**, mas com identificadores únicos `diferentes`. \n",
    "\n",
    "> ⚠️ Neste caso, não se pode descartar uma das supostas cópias. \n",
    "\n",
    "Existem ainda outras formas de duplicação de dados mais complexas, onde mais de um registro é associado à mesma observação, porém seus valores não são completamente idênticos. Por exemplo, nomes próprios com e sem abreviação ou omissão de algum dos sobrenomes. Essa duplicação parcial é **bem mais difícil de identificar**, pois requer entender se realmente os registros duplicados dizem respeito ao mesmo objeto. \n",
    "\n",
    "Nesses casos, uma solução comum é utilizar **funções de similaridade de strings.** \n",
    "\n",
    "Uma ferramenta poderosa para esse problema é a biblioteca Python _`FuzzyWuzzy`_, que usa a distância de Levenshtein para calcular as diferenças entre duas strings. \n",
    "\n",
    "No exemplo a seguir, nós utilizamos duas funções, `ratio()` e `partial_ratio()`, para encontrar cópias não idênticas de nomes de artistas. O código retorna dois prováveis casos de duplicação parcial.\n",
    "\n",
    "**No primeiro**, ao pesquisarmos a fundo, descobrimos que os dois nomes identificam duas artistas distintas. Portanto, não podemos remover essas supostas cópias. \n",
    "\n",
    "No entanto, **no segundo caso**, \"Red Velvet\" denomina o mesmo grupo feminino sul-coreano, porém o nome \"Red Velvet - Irene & Seulgi\" foi cadastrado na plataforma Spotify para representar a primeira subunidade do grupo (composto por Irene e Seulgi)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa os pacotes necessários\n",
    "from fuzzywuzzy import fuzz\n",
    "from itertools import combinations\n",
    "\n",
    "# Gera todas as combinações possíveis de dois elementos (nomes) da coluna 'name'\n",
    "combinacoes = combinations(df.name, 2)\n",
    "\n",
    "# Para cada tupla de nomes presente na lista de combinações,\n",
    "for nome_1, nome_2 in list(combinacoes):\n",
    "    \n",
    "    # Calcula a similaridade parcial dos dois nomes\n",
    "    partial_ratio = fuzz.partial_ratio(nome_1, nome_2)\n",
    "    \n",
    "    # Calcula a similaridade simples dos dois nomes\n",
    "    ratio = fuzz.ratio(nome_1, nome_2)\n",
    "    \n",
    "    # Se os nomes forem parcialmente iguais, porém não identicos,\n",
    "    if partial_ratio == 100 and ratio < 100 and ratio > 50:\n",
    "        \n",
    "        # Imprime os dois nomes e a pontuação das similaridades computadas\n",
    "        print(nome_1, ' | ', nome_2)\n",
    "        print(partial_ratio, ' | ', ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚠️  ERRO de Biblioteca ⚠️ \n",
    "\n",
    "Se o seguinte erro ocorrer, é devido à falta da biblioteca utilizada em seu ambiente python.\n",
    "\n",
    "![Erro de Lib](./img/Erro_fuzzy.png)\n",
    "\n",
    "Para resolver, podemos utilizar o comando de importação como no exemplo seguinte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala a biblioteca\n",
    "!pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desinstala a biblioteca\n",
    "!pip uninstall fuzzywuzzy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusão\n",
    "\n",
    "Este notebook apresentou como fazer a limpeza inicial de dados de dados.\n",
    "\n",
    "> 🔎 Se interessou? Dê uma olhada na documentação da biblioteca pandas para informações extras sobre funções de manipulação de dados: [User Guide](https://pandas.pydata.org/docs/user_guide/index.html)\n",
    "---\n",
    "\n",
    "A próxima parte ([4.Transformacao](../4.Transformacao/4.1.Integracao.ipynb)) apresenta como fazer a integração, transformação e redução de dados de várias fontes."
   ]
  }
 ],
 "metadata": {
  "julynter-results": {
   "filteredId": [],
   "filteredIndividual": [],
   "filteredRestart": [],
   "filteredType": [],
   "hash": "3e97903641083aa633eee000628a202ffd7c34e3",
   "visible": [
    {
     "cellId": "group",
     "hash": "c0de46ba034d710c2b2406a37ee6816be9634aa0",
     "reason": "This groups other lint messages",
     "reportId": "group",
     "reportType": "hiddenstate",
     "suggestion": null,
     "text": "Hidden State"
    },
    {
     "cellId": 2,
     "hash": "71d2b721a8a21948e50ef4dcebaa8038e9e7b930",
     "reason": "A skip in the execution count might indicate the presence of a hidden state caused by a cell that does not exist anymore. Hidden states might prevent cells from executing or producing the same results, hampering the reproducibility.",
     "reportId": "h4",
     "reportType": "hiddenstate",
     "suggestion": "Please consider re-running the notebook to guarantee the reproducibility.",
     "text": "Cell 2 skips the execution count"
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
