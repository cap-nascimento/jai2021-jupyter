{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando o pacote necessário\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redução de Dados\n",
    "\n",
    "Gerenciar e processar dados requer tempo, esforço e recursos, especialmente com grandes volumes de dados de várias fontes e em diferentes formatos. Para enfrentar tais desafios, técnicas de Redução de dados são aplicadas, auxiliando na análise de dados com alta dimensionalidade. Embora essenciais, essas técnicas geralmente são complexas, pois exigem amplo conhecimento para a escolha adequada de qual técnica utilizar. Assim, por simplicidade, essa seção apresenta apenas uma técnica, citando o texto para as demais. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redução de dimensionalidade\n",
    "\n",
    "É o processo de reduzir o número de dimensões de um conjunto de dados. Existem diferentes algoritmos de redução de dimensionalidade, mas nenhum método é único e ideal para todos os casos. Os diferentes algoritmos de redução de dimensionalidade podem ser divididos em duas categorias principais, Feature Selection e Feature Extraction. Nesta seção, nós exploraremos os principais métodos existentes nas duas categorias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Análise de Componentes Principais (PCA) é um dos métodos mais simples e, de longe, o mais comum para a redução da dimensionalidade. PCA é um algoritmo não supervisionado que cria combinações lineares dos atributos originais, classificadas em ordem de sua variância explicada. O próximo exemplo utiliza a biblioteca *scikit-learn* para importar o módulo `sklearn.decomposition` e a classe PCA para extrair os dois componentes principais `(n_components = 2)` do nosso conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_style": "center",
    "hide_input": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>-1.848746</td>\n",
       "      <td>-1.361856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>5.266716</td>\n",
       "      <td>1.026617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>0.451366</td>\n",
       "      <td>-1.150981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>-0.780667</td>\n",
       "      <td>0.101453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>-2.273107</td>\n",
       "      <td>0.817602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           PC1       PC2\n",
       "1279 -1.848746 -1.361856\n",
       "1280  5.266716  1.026617\n",
       "1281  0.451366 -1.150981\n",
       "1282 -0.780667  0.101453\n",
       "1283 -2.273107  0.817602"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Padronizando os dados de treino\n",
    "X = StandardScaler().fit_transform(X)\n",
    "# Calculando os dois componentes principais\n",
    "pca_resultado = PCA(n_components=2)\n",
    "df_pcs = pd.DataFrame(pca_resultado.fit_transform(X), columns=['PC1', 'PC2'])\n",
    "df_pcs.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O *DataFrame* resultante apresenta os valores dos dois componentes principais para todas as 1283 amostras. O conjunto de dados foi padronizado, utilizando a classe `StandardScaler`; do contrário, os atributos em maior escala dominariam os novos componentes principais. Após a extração dos componentes principais, o método fornece a quantidade de informações ou variação que cada componente principal mantém após projetar os dados em um subespaço de dimensão inferior. O primeiro componente principal detém 17,3% das informações, enquanto o segundo apenas 9% das informações. Ou seja, ao reduzir a dimensionalidade do conjunto de dados para duas dimensões, 73,7% das informações originais foram perdidas, conforme mostrado a seguir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variação explicada por componentes principais: [0.17349221 0.08862283]\n"
     ]
    }
   ],
   "source": [
    "print('Variação explicada por componentes principais: {}'.format(\n",
    "    pca_resultado.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "Este notebook apresentou como reduzir a dimensionalidade dos dados.\n",
    "\n",
    "Este foi o fim desta parte do tutorial sobre prepação de dados. A próxima parte ([4.Ciencia.de.Dados](../4.Ciencia.de.Dados/4.1.Analise.Exploratoria.ipynb)) apresentará como fazer analise exploratória dos dados como parte da ciência de dados.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "julynter-results": {
   "filteredId": [],
   "filteredIndividual": [],
   "filteredRestart": [],
   "filteredType": [],
   "hash": "42e8b7591aa2173a7b72b5ae2fb6718ece06becc",
   "visible": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
